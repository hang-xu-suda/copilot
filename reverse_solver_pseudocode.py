"""
åå‘Label-Settingæ±‚è§£å™¨ - ä¸¥æ ¼æŒ‰ç…§ä¼ªä»£ç å®ç°ï¼ˆå®Œæ•´ç‰ˆï¼‰
åŒ…å«è¯¦ç»†çš„æ¦‚ç‡è®¡ç®—è¯´æ˜
"""

import numpy as np
import heapq
import time
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass, field


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®ç»“æ„å®šä¹‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class AlphaDiscreteDistribution:
    """Î±ç¦»æ•£åˆ†å¸ƒç±»"""
    values: List[float]
    L1: int
    
    def __init__(self, values: List[float], L1: int):
        if len(values) != L1:
            raise ValueError(f"æœŸæœ›{L1}ä¸ªå€¼ï¼Œå®é™…å¾—åˆ°{len(values)}ä¸ª")
        self.L1 = L1
        self.values = sorted(values)
    
    def get_quantile(self, alpha: float) -> float:
        """è·å–Î±åˆ†ä½æ•°ï¼ˆçº¿æ€§æ’å€¼ï¼‰"""
        if alpha <= 0:
            return self.values[0]
        if alpha >= 1:
            return self.values[-1]
        
        index = alpha * (self.L1 - 1)
        lower_idx = int(np.floor(index))
        upper_idx = min(lower_idx + 1, self.L1 - 1)
        
        if lower_idx == upper_idx:
            return self.values[lower_idx]
        
        weight = index - lower_idx
        return self.values[lower_idx] * (1 - weight) + self.values[upper_idx] * weight
    
    def get_mean(self) -> float:
        return np.mean(self.values)
    
    def get_std(self) -> float:
        return np.std(self.values)
    
    def get_median(self) -> float:
        return np.median(self.values)
    
    def get_variance(self) -> float:
        """è·å–æ–¹å·®"""
        return np.var(self.values)
    
    def reverse_convolve(self,
                    get_link_dist_func,
                    predecessor: int,
                    current: int,
                    time_intervals_per_day: int,
                    L2: int) -> 'AlphaDiscreteDistribution':
        """
        åå‘å·ç§¯ï¼ˆåŸºäºç²¾ç¡®æ¦‚ç‡è®¡ç®—ï¼Œä¸é‡‡æ ·ï¼‰
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        1.éå†æ‰€æœ‰å¯èƒ½çš„å‡ºå‘æ—¶é—´ t_dep
        2.å¯¹æ¯ä¸ª t_depï¼Œè®¡ç®—å…¶å¯¹åº”çš„æ—¶é—´ç‰‡ slot_dep
        3.å¯¹æ¯ä¸ªåˆ°è¾¾æ—¶é—´ t_arrï¼Œè®¡ç®—æ‰€éœ€æ—…è¡Œæ—¶é—´ k = t_arr - t_dep
        4.ä» D_uv(slot_dep) ç›´æ¥æŸ¥è¯¢ P(k)ï¼Œè€Œéé‡‡æ ·
        5.ç´¯åŠ ï¼šP(t_dep) += P(t_arr) Ã— P(k | slot_dep)
        
        Args:
            get_link_dist_func: è·å–é“¾è·¯åˆ†å¸ƒçš„å‡½æ•°
            predecessor: å‰é©±èŠ‚ç‚¹u
            current: å½“å‰èŠ‚ç‚¹v
            time_intervals_per_day: æ¯å¤©æ—¶é—´ç‰‡æ•°
            L2: æœªä½¿ç”¨ï¼ˆä¿æŒæ¥å£å…¼å®¹ï¼‰
            
        Returns:
            å‡ºå‘æ—¶é—´åˆ†å¸ƒ A(u)
        """
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤1: è·å–å¯ç”¨æ—¶é—´ç‰‡å’Œæ—…è¡Œæ—¶é—´èŒƒå›´
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        available_slots = self._get_available_slots(get_link_dist_func, predecessor, current)
        
        if not available_slots:
            raise ValueError(f"è¾¹({predecessor}, {current})æ²¡æœ‰é“¾è·¯åˆ†å¸ƒæ•°æ®")
        
        # ä¼°ç®—æ—…è¡Œæ—¶é—´èŒƒå›´ï¼ˆç”¨äºç¡®å®šå‡ºå‘æ—¶é—´æœç´¢èŒƒå›´ï¼‰
        min_travel = float('inf')
        max_travel = 0
        
        for slot in available_slots:
            D_slot = get_link_dist_func(predecessor, current, slot)
            if D_slot and D_slot.times:
                min_travel = min(min_travel, min(D_slot.times))
                max_travel = max(max_travel, max(D_slot.times))
        
        if min_travel == float('inf'):
            raise ValueError(f"æ— æ³•è·å–è¾¹({predecessor}, {current})çš„æ—…è¡Œæ—¶é—´èŒƒå›´")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤2: ç¡®å®šå‡ºå‘æ—¶é—´æœç´¢èŒƒå›´
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ä»æ‰€æœ‰åˆ°è¾¾æ—¶é—´æ¨å¯¼å‡ºå‘æ—¶é—´çš„å¯èƒ½èŒƒå›´
        min_arrival = min(self.values)
        max_arrival = max(self.values)
        
        min_departure = min_arrival - max_travel
        max_departure = max_arrival - min_travel
        
        # ç¦»æ•£åŒ–å‡ºå‘æ—¶é—´ï¼šæ­¥é•¿ä¸º10ï¼ˆ1åˆ†é’Ÿï¼‰
        # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ­¥é•¿ï¼Œè¶Šå°è¶Šç²¾ç¡®ä½†è®¡ç®—é‡è¶Šå¤§
        step = 1  # 0.1åˆ†é’Ÿå•ä½ Ã— 10 = 1åˆ†é’Ÿ
        
        # ç”Ÿæˆå€™é€‰å‡ºå‘æ—¶é—´
        candidate_departures = np.arange(
            int(min_departure),
            int(max_departure) + step,
            step
        )
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤3: æ„å»ºåˆ°è¾¾æ—¶é—´çš„æ¦‚ç‡åˆ†å¸ƒ
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # å°† self.values (L1ä¸ªæ ·æœ¬) è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        arrival_probs = {}
        for t_arr in self.values:
            if t_arr not in arrival_probs:
                arrival_probs[t_arr] = 0
            arrival_probs[t_arr] += 1.0 / self.L1
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤4: å¯¹æ¯ä¸ªå€™é€‰å‡ºå‘æ—¶é—´ï¼Œè®¡ç®—å…¶æ¦‚ç‡
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        departure_probs = {}
        
        for t_dep in candidate_departures:
            # ç¡®å®šå‡ºå‘æ—¶é—´å¯¹åº”çš„æ—¶é—´ç‰‡
            slot_dep = int(t_dep / 10) % time_intervals_per_day
            
            # è·å–è¯¥æ—¶é—´ç‰‡çš„è·¯æ®µåˆ†å¸ƒ
            D_slot = get_link_dist_func(predecessor, current, slot_dep)
            
            if D_slot is None:
                # å¦‚æœç²¾ç¡®æ—¶é—´ç‰‡æ²¡æœ‰åˆ†å¸ƒï¼Œå°è¯•æœ€è¿‘çš„æ—¶é—´ç‰‡
                nearest_slot = self._find_nearest_slot(
                    slot_dep, available_slots, time_intervals_per_day
                )
                D_slot = get_link_dist_func(predecessor, current, nearest_slot)
            
            if D_slot is None:
                continue
            
            # è®¡ç®— P(t_dep)
            prob_t_dep = 0.0
            
            for t_arr, prob_arr in arrival_probs.items():
                # è®¡ç®—æ‰€éœ€æ—…è¡Œæ—¶é—´
                required_travel_time = t_arr - t_dep
                
                # æŸ¥è¯¢è¯¥æ—…è¡Œæ—¶é—´çš„æ¦‚ç‡
                prob_travel = D_slot.get_probability(required_travel_time)
                
                if prob_travel > 0:
                    # P(t_dep) += P(t_arr) Ã— P(travel_time = t_arr - t_dep | slot_dep)
                    prob_t_dep += prob_arr * prob_travel
            
            if prob_t_dep > 0:
                departure_probs[t_dep] = prob_t_dep
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤5: å½’ä¸€åŒ–å¹¶æ„é€ æ–°åˆ†å¸ƒ
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if not departure_probs:
            raise ValueError(f"åå‘å·ç§¯å¤±è´¥ï¼šæ— æœ‰æ•ˆå‡ºå‘æ—¶é—´æ¦‚ç‡")
        
        # å½’ä¸€åŒ–
        total_prob = sum(departure_probs.values())
        if total_prob <= 0:
            raise ValueError(f"åå‘å·ç§¯å¤±è´¥ï¼šæ€»æ¦‚ç‡ä¸º0")
        
        for t in departure_probs:
            departure_probs[t] /= total_prob
        
        # è½¬æ¢ä¸ºæ•°ç»„
        times = np.array(sorted(departure_probs.keys()))
        probs = np.array([departure_probs[t] for t in times])
        
        # é‡æ–°å½’ä¸€åŒ–ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
        probs = probs / probs.sum()
        
        # æ–¹æ³•1: æŒ‰æ¦‚ç‡åŠ æƒé‡‡æ ·L1ä¸ªå€¼
        sampled_indices = np.random.choice(len(times), size=self.L1, replace=True, p=probs)
        sampled_times = times[sampled_indices]
        
        # æ–¹æ³•2ï¼ˆå¯é€‰ï¼‰: æŒ‰ç´¯ç§¯åˆ†ä½æ•°é€‰æ‹©L1ä¸ªç¡®å®šæ€§ä»£è¡¨å€¼
        # cdf = np.cumsum(probs)
        # quantiles = np.linspace(1/(self.L1+1), self.L1/(self.L1+1), self.L1)
        # sampled_times = np.interp(quantiles, cdf, times)
        
        sampled_times.sort()
        
        return AlphaDiscreteDistribution(sampled_times.tolist(), self.L1)


    def _find_nearest_slot(self, target_slot: int, available_slots: List[int],
                        time_intervals_per_day: int) -> int:
        """æ‰¾åˆ°æœ€è¿‘çš„æ—¶é—´ç‰‡"""
        min_dist = float('inf')
        best_slot = available_slots[0]
        
        for slot in available_slots:
            dist = abs(slot - target_slot)
            cyclic_dist = min(dist, time_intervals_per_day - dist)
            
            if cyclic_dist < min_dist:
                min_dist = cyclic_dist
                best_slot = slot
        
        return best_slot
    
    def _get_available_slots(self, get_link_dist_func, u: int, v: int) -> List[int]:
        if not hasattr(AlphaDiscreteDistribution, '_slot_cache'):
            AlphaDiscreteDistribution._slot_cache = {}
        cache_key = (u, v)
        if cache_key in AlphaDiscreteDistribution._slot_cache:
            return AlphaDiscreteDistribution._slot_cache[cache_key]
        available = []
        try:
            link_distributions = get_link_dist_func.__self__.link_distributions
            for (link_u, link_v, slot) in link_distributions.keys():
                if link_u == u and link_v == v:
                    available.append(slot)
        except AttributeError:
            raise ValueError("æ— æ³•è®¿é—®é“¾è·¯åˆ†å¸ƒæ•°æ®")
        result = sorted(set(available))
        AlphaDiscreteDistribution._slot_cache[cache_key] = result
        return result
    
    def _get_slots_in_range(self, slot_min: int, slot_max: int,
                           available_slots: List[int],
                           time_intervals_per_day: int) -> List[int]:
        """è·å–èŒƒå›´[slot_min, slot_max]å†…çš„å€™é€‰æ—¶é—´ç‰‡"""
        candidate_slots = []
        
        if slot_min <= slot_max:
            # æ­£å¸¸èŒƒå›´
            for slot in available_slots:
                if slot_min <= slot <= slot_max:
                    candidate_slots.append(slot)
        else:
            # è·¨å¤©èŒƒå›´
            for slot in available_slots:
                if slot >= slot_min or slot <= slot_max:
                    candidate_slots.append(slot)
        
        return sorted(candidate_slots)


@dataclass
class LinkTimeDistribution:
    """è·¯æ®µæ—…è¡Œæ—¶é—´åˆ†å¸ƒ"""
    time_prob: Dict[int, float]
    times: List[int]
    cdf: List[float]
    time_slot: int
    
    def __init__(self, time_prob_dict: Dict[int, float], time_slot: int = None):
        if not time_prob_dict:
            raise ValueError("é“¾è·¯åˆ†å¸ƒä¸èƒ½ä¸ºç©º")
        
        total_prob = sum(time_prob_dict.values())
        self.time_prob = {t: p/total_prob for t, p in time_prob_dict.items()}
        self.time_slot = time_slot
        
        sorted_times = sorted(self.time_prob.keys())
        self.times = sorted_times
        
        cumulative = 0.0
        self.cdf = []
        for t in sorted_times:
            cumulative += self.time_prob[t]
            self.cdf.append(cumulative)
    
    def sample_L2_times(self, reference_time: int, L2: int) -> List[int]:
        """é‡‡æ ·L2ä¸ªæ—…è¡Œæ—¶é—´ï¼ˆé€†CDFæ–¹æ³•ï¼‰"""
        samples = []
        for i in range(1, L2 + 1):
            quantile = i / (L2 + 1)
            sample = self._inverse_cdf(quantile)
            samples.append(sample)
        return sorted(samples)
    
    def _inverse_cdf(self, quantile: float) -> int:
        """é€†CDFï¼ˆçº¿æ€§æ’å€¼ï¼‰"""
        if quantile <= 0:
            return self.times[0]
        if quantile >= 1:
            return self.times[-1]
        
        for i, cdf_val in enumerate(self.cdf):
            if cdf_val >= quantile:
                if i == 0:
                    return self.times[0]
                
                lower_cdf = self.cdf[i-1] if i > 0 else 0
                upper_cdf = cdf_val
                lower_time = self.times[i-1] if i > 0 else self.times[0]
                upper_time = self.times[i]
                
                if upper_cdf > lower_cdf:
                    weight = (quantile - lower_cdf) / (upper_cdf - lower_cdf)
                else:
                    weight = 0.5
                
                return int(round(lower_time + weight * (upper_time - lower_time)))
        
        return self.times[-1]
    
    def get_probability(self, travel_time: float) -> float:
        """
        è·å–æŒ‡å®šæ—…è¡Œæ—¶é—´çš„æ¦‚ç‡
        
        Args:
            travel_time: æ—…è¡Œæ—¶é—´ï¼ˆ0.1åˆ†é’Ÿå•ä½ï¼‰
            
        Returns:
            è¯¥æ—…è¡Œæ—¶é—´çš„æ¦‚ç‡ï¼ˆå¦‚æœä¸åœ¨æ”¯æŒé›†ä¸­ï¼Œå¯é€‰æ‹©æ’å€¼æˆ–è¿”å›0ï¼‰
        """
        # è½¬æ¢ä¸ºæ•´æ•°ï¼ˆä¸å­˜å‚¨çš„é”®åŒ¹é…ï¼‰
        travel_time_int = int(round(travel_time))
        
        # ç²¾ç¡®åŒ¹é…
        if travel_time_int in self.time_prob:
            return self.time_prob[travel_time_int]
        
        # å¯é€‰ï¼šçº¿æ€§æ’å€¼ï¼ˆå¦‚æœéœ€è¦æ›´å¹³æ»‘çš„ç»“æœï¼‰
        if self.times:
            min_time = self.times[0]
            max_time = self.times[-1]
            
            if travel_time_int < min_time or travel_time_int > max_time:
                return 0.0
            
            # æ‰¾åˆ°ç›¸é‚»çš„ä¸¤ä¸ªç‚¹
            for i in range(len(self.times) - 1):
                if self.times[i] <= travel_time_int <= self.times[i+1]:
                    # çº¿æ€§æ’å€¼
                    t_lower = self.times[i]
                    t_upper = self.times[i+1]
                    p_lower = self.time_prob[t_lower]
                    p_upper = self.time_prob[t_upper]
                    
                    if t_upper == t_lower:
                        return p_lower
                    
                    weight = (travel_time_int - t_lower) / (t_upper - t_lower)
                    return p_lower * (1 - weight) + p_upper * weight
        
        return 0.0

    def get_mean(self) -> float:
        return sum(t * p for t, p in self.time_prob.items())
    
    def get_std(self) -> float:
        mean = self.get_mean()
        variance = sum(p * (t - mean)**2 for t, p in self.time_prob.items())
        return np.sqrt(variance)


@dataclass
class ReverseLabel:
    """åå‘æœç´¢æ ‡ç­¾"""
    node_id: int
    distribution: AlphaDiscreteDistribution
    path: List[int]
    cost: float
    quantile_cache: Dict[float, float] = field(default_factory=dict)
    
    def __post_init__(self):
        # é¢„è®¡ç®—å¸¸ç”¨åˆ†ä½æ•°
        for q in [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]:
            self.quantile_cache[q] = self.distribution.get_quantile(q)
        
        # é¢„è®¡ç®—å‡å€¼å’Œæ–¹å·®
        self.mean_cache = self.distribution.get_mean()
        self.variance_cache = self.distribution.get_variance()
    
    def __lt__(self, other):
        return self.cost > other.cost
    
    def get_cached_quantile(self, alpha: float) -> float:
        if alpha in self.quantile_cache:
            return self.quantile_cache[alpha]
        value = self.distribution.get_quantile(alpha)
        self.quantile_cache[alpha] = value
        return value
    
    def dominates_weak(self, other: 'ReverseLabel', alpha: float, epsilon: float = 1e-6) -> bool:
        """
        å¼±æ”¯é…æ£€æŸ¥ï¼ˆä¿å®ˆå‰ªæï¼‰
        
        ä»…åœ¨ä»¥ä¸‹æƒ…å†µæ”¯é…ï¼š
        1.ä¸»ç›®æ ‡æ˜¾è‘—æ›´ä¼˜ + æ¬¡è¦ç›®æ ‡ä¸æ›´å·®
        2.ä¸»ç›®æ ‡ä¸æ›´å·® + ä¸¤ä¸ªæ¬¡è¦ç›®æ ‡éƒ½æ˜¾è‘—æ›´ä¼˜
        """
        if self.node_id != other.node_id:
            return False
        
        self_q = self.distribution.get_quantile(1 - alpha)
        other_q = other.distribution.get_quantile(1 - alpha)
        
        # ç­–ç•¥Aï¼šä¸»ç›®æ ‡æ˜¾è‘—æ›´ä¼˜
        if self_q > other_q + epsilon:
            # æ¬¡è¦ç›®æ ‡å¿…é¡»ä¸æ›´å·®
            if (self.mean_cache >= other.mean_cache - epsilon and
                self.variance_cache <= other.variance_cache + epsilon):
                return True
        
        # ç­–ç•¥Bï¼šä¸»ç›®æ ‡ç›¸è¿‘ï¼Œä½†æ¬¡è¦ç›®æ ‡æ˜¾è‘—æ›´ä¼˜
        if abs(self_q - other_q) <= epsilon:
            # ä¸¤ä¸ªæ¬¡è¦ç›®æ ‡éƒ½æ˜¾è‘—æ›´ä¼˜
            if (self.mean_cache > other.mean_cache + epsilon and
                self.variance_cache < other.variance_cache - epsilon):
                return True
            
            # æˆ–è€…ï¼šå‡å€¼æ˜¾è‘—æ›´ä¼˜ + æ–¹å·®ä¸æ›´å·®
            if (self.mean_cache > other.mean_cache + epsilon and
                self.variance_cache <= other.variance_cache + epsilon):
                return True
            
            # æˆ–è€…ï¼šæ–¹å·®æ˜¾è‘—æ›´ä¼˜ + å‡å€¼ä¸æ›´å·®  
            if (self.variance_cache < other.variance_cache - epsilon and
                self.mean_cache >= other.mean_cache - epsilon):
                return True
        
        return False
    
    def dominates(self, other: 'ReverseLabel', alpha: float, epsilon: float = 1e-6) -> bool:
        """ç»Ÿä¸€æ¥å£"""
        return self.dominates_weak(other, alpha, epsilon)



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# åå‘æ±‚è§£å™¨ä¸»ç±»
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ReverseLabelSettingSolver:
    """åå‘Label-Settingæ±‚è§£å™¨ï¼ˆå®Œæ•´ç‰ˆï¼‰"""
    
    def __init__(self, G, sparse_data, node_to_index, scenario_dates,
                 scenario_probs, time_intervals_per_day,
                 L1: int = 50, L2: int = 10,
                 verbose: bool = False,
                 max_labels_per_node: int = 20):
        """åˆå§‹åŒ–"""
        self.G = G
        self.sparse_data = sparse_data
        self.node_to_index = node_to_index
        self.index_to_node = {v: k for k, v in node_to_index.items()}
        self.scenario_dates = scenario_dates
        self.scenario_probs = scenario_probs
        self.time_intervals_per_day = time_intervals_per_day
        self.n_scenarios = len(scenario_dates)
        
        self.L1 = L1
        self.L2 = L2
        self.verbose = verbose

        self.max_labels_per_node = max_labels_per_node
        
        print(f"\n{'='*70}")
        print(f"åˆå§‹åŒ–åå‘Label-Settingæ±‚è§£å™¨ï¼ˆå®Œæ•´ç‰ˆï¼‰")
        print(f"{'='*70}")
        print(f"  ç®—æ³•: åå‘Label-Setting with æ¦‚ç‡æƒé‡")
        print(f"  é—®é¢˜: é¢„ç•™æ—¶é—´é¢„ç®—")
        print(f"  å‚æ•°: L1={L1}, L2={L2}")
        print(f"  è¯¦ç»†è¾“å‡º: {'å¼€å¯' if verbose else 'å…³é—­'}")
        
        # æ„å»ºé‚»æ¥è¡¨
        self.adj_list = defaultdict(list)
        self.reverse_adj_list = defaultdict(list)
        self._build_adjacency_lists()
        
        # é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ
        self.link_distributions = {}
        self._precompute_link_distributions()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = defaultdict(int)
        self.origin_labels_history = []
        
        print(f"\nâœ“ åˆå§‹åŒ–å®Œæˆ")
        print(f"{'='*70}\n")
    
    def _build_adjacency_lists(self):
        """æ„å»ºé‚»æ¥è¡¨"""
        print(f"  [1/2] æ„å»ºé‚»æ¥è¡¨...")
        start_time = time.time()
        
        edges_set = set()
        for (scenario_idx, time_idx, from_idx, to_idx) in self.sparse_data.keys():
            if scenario_idx < self.n_scenarios:
                from_node = self.index_to_node[from_idx]
                to_node = self.index_to_node[to_idx]
                edges_set.add((from_node, to_node))
        
        for from_node, to_node in edges_set:
            self.adj_list[from_node].append(to_node)
            self.reverse_adj_list[to_node].append(from_node)
        
        elapsed = time.time() - start_time
        print(f"      âœ“ å®Œæˆ (ç”¨æ—¶ {elapsed:.2f}s) - {len(edges_set):,} æ¡è¾¹")
    
    def _precompute_link_distributions(self):
        """é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ"""
        print(f"  [2/2] é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ...")
        start_time = time.time()
        
        link_time_data = defaultdict(list)
        
        for (scenario_idx, time_idx, from_idx, to_idx), travel_time_minutes in self.sparse_data.items():
            if scenario_idx >= self.n_scenarios:
                continue
            
            from_node = self.index_to_node[from_idx]
            to_node = self.index_to_node[to_idx]
            travel_time_01min = int(travel_time_minutes * 10)
            
            link_time_data[(from_node, to_node, time_idx)].append(travel_time_01min)
        
        distribution_count = 0
        for (u, v, t), times in link_time_data.items():
            time_counts = defaultdict(int)
            for time_val in times:
                time_counts[time_val] += 1
            
            total = len(times)
            time_prob = {time_val: count/total for time_val, count in time_counts.items()}
            
            try:
                self.link_distributions[(u, v, t)] = LinkTimeDistribution(time_prob, time_slot=t)
                distribution_count += 1
            except ValueError:
                continue
        
        elapsed = time.time() - start_time
        print(f"      âœ“ å®Œæˆ (ç”¨æ—¶ {elapsed:.2f}s) - {distribution_count:,} ä¸ªåˆ†å¸ƒ")
    
    def _get_link_distribution_at_slot(self, u: int, v: int, slot: int) -> Optional[LinkTimeDistribution]:
        """è·å–æŒ‡å®šå‡ºå‘æ—¶é—´ç‰‡çš„é“¾è·¯åˆ†å¸ƒ"""
        if (u, v, slot) in self.link_distributions:
            return self.link_distributions[(u, v, slot)]
        
        # å®¹å·®åŒ¹é…
        tolerance = 5
        candidates = []
        
        for (link_u, link_v, link_t) in self.link_distributions.keys():
            if link_u == u and link_v == v:
                diff = abs(link_t - slot)
                cyclic_diff = min(diff, self.time_intervals_per_day - diff)
                
                if cyclic_diff <= tolerance:
                    candidates.append((link_t, cyclic_diff))
        
        if candidates:
            best_slot = min(candidates, key=lambda x: x[1])[0]
            return self.link_distributions[(u, v, best_slot)]
        
        return None
    
    def solve_k_paths(self, origin: int, destination: int, target_arrival_time: int,
                     alpha: float, K: int = 10, max_labels:  int = 100000,
                     print_interval: int = 100) -> Dict: 
        """
        K-Paths åå‘æ±‚è§£
        
        Args: 
            origin: èµ·ç‚¹
            destination: ç»ˆç‚¹
            target_arrival_time: ç›®æ ‡åˆ°è¾¾æ—¶é—´
            alpha: å¯é æ€§å‚æ•°
            K: å€™é€‰è·¯å¾„æ•°é‡
            max_labels: æœ€å¤§æ ‡ç­¾æ•°
            print_interval: æ‰“å°é—´éš”
        
        Returns:
            åŒ…å«Kæ¡å€™é€‰è·¯å¾„çš„ç»“æœå­—å…¸
        """
        
        print(f"\n{'='*70}")
        print(f"åå‘Label-Settingæ±‚è§£ï¼ˆK-Pathsç‰ˆæœ¬ï¼‰")
        print(f"{'='*70}")
        print(f"  èµ·ç‚¹: {origin}")
        print(f"  ç»ˆç‚¹: {destination}")
        print(f"  ç›®æ ‡åˆ°è¾¾:  {target_arrival_time/10:.1f}åˆ†")
        print(f"  å¯é æ€§: Î±={alpha*100:.1f}%")
        print(f"  å€™é€‰è·¯å¾„æ•°:  K={K}")
        print(f"{'='*70}\n")
        
        start_time = time.time()
        
        # åˆå§‹åŒ–
        open_labels = []
        node_labels = defaultdict(list)
        origin_candidates = []  # âœ… å­˜å‚¨æ‰€æœ‰åˆ°è¾¾èµ·ç‚¹çš„å€™é€‰æ ‡ç­¾
        
        # åˆå§‹æ ‡ç­¾
        init_dist = AlphaDiscreteDistribution([target_arrival_time] * self.L1, self.L1)
        init_label = ReverseLabel(destination, init_dist, [destination], target_arrival_time)
        
        heapq.heappush(open_labels, init_label)
        node_labels[destination].append(init_label)
        self.stats = defaultdict(int)
        self.stats['labels_generated'] = 1
        
        print(f"å¼€å§‹æœç´¢ K={K} æ¡å€™é€‰è·¯å¾„...\n")
        
        iteration = 0
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ä¸»å¾ªç¯ï¼šæ‰¾åˆ°Kæ¡åˆ°è¾¾èµ·ç‚¹çš„è·¯å¾„
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        while open_labels and self.stats['labels_generated'] < max_labels:
            iteration += 1
            current_label = heapq.heappop(open_labels)
            
            if self.verbose and (iteration % print_interval == 0 or iteration <= 5):
                print(f"  è¿­ä»£#{iteration}: èŠ‚ç‚¹{current_label.node_id}, "
                      f"cost={current_label.cost/10:.1f}åˆ†, "
                      f"å€™é€‰æ•°={len(origin_candidates)}")
            
            # âœ… åˆ°è¾¾èµ·ç‚¹ï¼šä¿å­˜ä¸ºå€™é€‰è·¯å¾„
            if current_label.node_id == origin:
                latest_departure = current_label.distribution.get_quantile(1 - alpha)
                expected_departure = current_label.mean_cache
                
                # ä¿å­˜å€™é€‰è·¯å¾„ä¿¡æ¯
                candidate_info = {
                    'iteration': iteration,
                    'path': list(reversed(current_label. path)),
                    'distribution': current_label.distribution,
                    'latest_departure': latest_departure,
                    'expected_departure': expected_departure,
                    'median_departure': current_label.distribution.get_median(),
                    'std_departure': np.sqrt(current_label.variance_cache),
                    'variance': current_label.variance_cache,
                    'label': current_label,
                    'alpha': alpha,
                    'rank': None,
                    'is_best':  False  # â† æ·»åŠ è¿™ä¸ªå­—æ®µ
                }
                
                origin_candidates.append(candidate_info)
                
                print(f"  ğŸ¯ æ‰¾åˆ°å€™é€‰è·¯å¾„#{len(origin_candidates)}  è¿­ä»£#{iteration}, "
                      f"Q_{{1-Î±}}={latest_departure/10:.1f}åˆ†, "
                      f"Mean={expected_departure/10:.1f}åˆ†, "
                      f"è·¯å¾„é•¿åº¦={len(current_label.path)}")
                
                # âœ… æ‰¾åˆ°Kæ¡è·¯å¾„åç»§ç»­æœç´¢ï¼ˆç¡®ä¿æ¢ç´¢å……åˆ†ï¼‰
                if len(origin_candidates) >= K:
                    # å¯ä»¥é€‰æ‹©ï¼š
                    # é€‰é¡¹Aï¼šç«‹å³åœæ­¢ï¼ˆå¿«é€Ÿï¼‰
                    # é€‰é¡¹Bï¼šç»§ç»­æœç´¢ä¸€æ®µæ—¶é—´ï¼ˆæ›´å…¨é¢ï¼‰
                    
                    # è¿™é‡Œä½¿ç”¨é€‰é¡¹Bï¼šç»§ç»­æœç´¢ï¼Œä½†æœ‰ä¸Šé™
                    if len(origin_candidates) >= K * 2:  # æ‰¾åˆ°2Kæ¡ååœæ­¢
                        print(f"\n  âœ“ å·²æ‰¾åˆ° {len(origin_candidates)} æ¡å€™é€‰è·¯å¾„ï¼Œåœæ­¢æœç´¢\n")
                        break
                
                # ç»§ç»­æœç´¢å…¶ä»–è·¯å¾„
                continue
            
            # æ”¯é…æ€§æ£€æŸ¥ï¼ˆè¾ƒå®½æ¾ï¼Œä¿ç•™å¤šæ ·æ€§ï¼‰
            if self._is_dominated(current_label, node_labels[current_label.node_id], alpha):
                self.stats['labels_dominated'] += 1
                continue
            
            self.stats['labels_extended'] += 1
            
            # åå‘æ‰©å±•
            if current_label.node_id not in self.reverse_adj_list:
                continue
            
            for predecessor in self.reverse_adj_list[current_label.node_id]:
                if predecessor in current_label.path:
                    continue
                
                # åå‘å·ç§¯
                try:
                    def get_link_dist(u, v, slot):
                        return self._get_link_distribution_at_slot(u, v, slot)
                    
                    get_link_dist.__self__ = self
                    
                    new_dist = current_label.distribution.reverse_convolve(
                        get_link_dist_func=get_link_dist,
                        predecessor=predecessor,
                        current=current_label.node_id,
                        time_intervals_per_day=self.time_intervals_per_day,
                        L2=self.L2
                    )
                    
                    self.stats['convolutions'] += 1
                    
                except Exception as e:
                    if self.verbose and iteration <= 10:
                        print(f"      âš  å·ç§¯å¤±è´¥: {e}")
                    continue
                
                new_cost = new_dist.get_quantile(1 - alpha)
                new_label = ReverseLabel(predecessor, new_dist, 
                                        current_label.path + [predecessor], new_cost)
                
                self.stats['labels_generated'] += 1
                
                # æ”¯é…æ€§å‰ªæ
                if self._is_dominated(new_label, node_labels[predecessor], alpha):
                    self.stats['labels_dominated'] += 1
                    continue
                
                # åå‘å‰ªæ
                original_count = len(node_labels[predecessor])
                node_labels[predecessor] = [
                    old for old in node_labels[predecessor]
                    if not new_label.dominates_weak(old, alpha)
                ]
                self.stats['labels_dominated'] += (original_count - len(node_labels[predecessor]))
                
                node_labels[predecessor].append(new_label)
                node_labels[predecessor] = self._prune_labels(node_labels[predecessor], alpha)
                heapq.heappush(open_labels, new_label)
            
            # è¿›åº¦æ˜¾ç¤º
            if not self.verbose and iteration % 100 == 0:
                print(f"  è¿›åº¦: è¿­ä»£#{iteration}, ç”Ÿæˆ{self.stats['labels_generated']: ,}, "
                      f"å€™é€‰{len(origin_candidates)}, "
                      f"å‰ªæ{self.stats['labels_dominated']:,}", end='\r')
        
        total_time = time.time() - start_time
        
        print(f"\n\n{'='*70}")
        print(f"æœç´¢å®Œæˆ")
        print(f"{'='*70}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤2ï¼šå¯¹Kæ¡å€™é€‰è·¯å¾„æ’åº
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        if not origin_candidates:
            print(f"âœ— æœªæ‰¾åˆ°åˆ°è¾¾èµ·ç‚¹çš„è·¯å¾„")
            return {
                'success': False,
                'total_time': total_time,
                'iterations': iteration,
                'stats':  dict(self.stats),
                'num_candidates': 0
            }
        
        print(f"\næ‰¾åˆ° {len(origin_candidates)} æ¡å€™é€‰è·¯å¾„")
        print(f"å¼€å§‹æ’åºå’Œæ¯”è¾ƒ...\n")
        
        # âœ… å¤šç›®æ ‡æ’åºï¼šä¸»è¦Q_{1-Î±}ï¼Œæ¬¡è¦Meanï¼Œå†æ¬¡è¦-Var
        def rank_score(candidate):
            return (
                candidate['latest_departure'],      # ä¸»ç›®æ ‡ï¼šQ_{1-Î±}ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
                candidate['expected_departure'],    # æ¬¡è¦ï¼šå‡å€¼ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
                -candidate['variance']              # å†æ¬¡è¦ï¼šæ–¹å·®ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            )
        
        # æ’åºï¼šä»æœ€ä¼˜åˆ°æœ€å·®
        sorted_candidates = sorted(origin_candidates, key=rank_score, reverse=True)
        
        # è®¾ç½®æ’å
        for rank, candidate in enumerate(sorted_candidates, 1):
            candidate['rank'] = rank
            candidate['is_best'] = (rank == 1)  # â† æ’åç¬¬1çš„æ ‡è®°ä¸ºæœ€ä¼˜
        
        # å–å‰Kæ¡
        top_k_candidates = sorted_candidates[:K]
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤3ï¼šè¾“å‡ºç»“æœ
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        best_candidate = top_k_candidates[0]
        
        print(f"{'='*70}")
        print(f"Top-{len(top_k_candidates)} å€™é€‰è·¯å¾„å¯¹æ¯”")
        print(f"{'='*70}\n")
        
        print(f"{'æ’å':<6} {'Q_{{1-Î±}}(åˆ†)':<15} {'Mean(åˆ†)':<15} {'Std(åˆ†)':<12} {'è·¯å¾„é•¿åº¦':<10}")
        print(f"{'-'*70}")
        
        for candidate in top_k_candidates: 
            print(f"{candidate['rank']:<6} "
                  f"{candidate['latest_departure']/10:<15.1f} "
                  f"{candidate['expected_departure']/10:<15.1f} "
                  f"{candidate['std_departure']/10:<12.2f} "
                  f"{len(candidate['path']):<10}")
        
        print(f"\n{'='*70}")
        print(f"âœ“ æœ€ä¼˜è·¯å¾„ï¼ˆæ’å#1ï¼‰")
        print(f"{'='*70}")
        print(f"\n  è·¯å¾„:  {self._format_path(best_candidate['path'])}")
        print(f"  é•¿åº¦: {len(best_candidate['path'])} ä¸ªèŠ‚ç‚¹")
        print(f"\n  æ—¶é—´:")
        print(f"    ç›®æ ‡åˆ°è¾¾: {target_arrival_time/10:.1f}åˆ†")
        print(f"    æœ€æ™šå‡ºå‘ (Î±={alpha}): {best_candidate['latest_departure']/10:.1f}åˆ†")
        print(f"    æœŸæœ›å‡ºå‘: {best_candidate['expected_departure']/10:.1f}åˆ†")
        print(f"    é¢„ç•™æ—¶é—´: {(target_arrival_time - best_candidate['latest_departure'])/10:.1f}åˆ†")
        print(f"    æ ‡å‡†å·®:  {best_candidate['std_departure']/10:.2f}åˆ†")
        print(f"\n  æ€§èƒ½:")
        print(f"    æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"    è¿­ä»£æ¬¡æ•°: {iteration}")
        print(f"    å€™é€‰è·¯å¾„æ•°: {len(origin_candidates)}")
        print(f"    ç”Ÿæˆæ ‡ç­¾:  {self.stats['labels_generated']: ,}")
        print(f"    å‰ªæç‡: {self.stats['labels_dominated']/self.stats['labels_generated']*100:.1f}%")
        print(f"{'='*70}\n")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ„å»ºè¿”å›ç»“æœ
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        result = {
            'success': True,
            # æœ€ä¼˜è·¯å¾„ä¿¡æ¯
            'path': best_candidate['path'],
            'latest_departure_time': best_candidate['latest_departure'],
            'expected_departure_time': best_candidate['expected_departure'],
            'median_departure_time': best_candidate['median_departure'],
            'std_departure_time': best_candidate['std_departure'],
            'reserved_time': target_arrival_time - best_candidate['latest_departure'],
            'distribution': best_candidate['distribution'],
            
            # Top-Kå€™é€‰è·¯å¾„
            'top_k_candidates': top_k_candidates,
            'num_candidates': len(origin_candidates),
            'all_candidates': sorted_candidates,
            
            # å…ƒä¿¡æ¯
            'total_time':  total_time,
            'iterations': iteration,
            'alpha': alpha,
            'K':  K,
            'origin':  origin,
            'destination': destination,
            'target_arrival_time': target_arrival_time,
            'stats': dict(self.stats)
        }
        
        return result

    def solve(self, origin: int, destination: int, target_arrival_time: int,
            alpha: float, max_labels: int = 100000,
            print_interval: int = 100,
            save_all_paths: bool = True) -> Dict: 
        """
        æ ‡å‡†solveæ¥å£ï¼ˆå…¼å®¹åŸä»£ç ï¼‰
        
        å†…éƒ¨è°ƒç”¨solve_k_pathsï¼ŒK=5ï¼ˆè¿”å›5æ¡å€™é€‰è·¯å¾„ï¼‰
        """
        result = self.solve_k_paths(
            origin, destination, target_arrival_time, alpha,
            K=5, max_labels=max_labels, print_interval=print_interval
        )
        
        # æ·»åŠ all_pathsç”¨äºå…¼å®¹
        if save_all_paths and 'all_candidates' in result:
            result['all_paths'] = result['all_candidates']
            result['num_candidate_paths'] = result['num_candidates']
        
        return result
    
    def _is_dominated(self, label: ReverseLabel, existing_labels: List[ReverseLabel], 
                     alpha: float) -> bool:
        """
        æ”¯é…æ€§æ£€æŸ¥ï¼ˆä¿å®ˆç‰ˆæœ¬ï¼‰
        
        ç­–ç•¥ï¼š
        1.å¦‚æœèŠ‚ç‚¹æ ‡ç­¾æ•° < max_labels_per_nodeï¼šä¸å‰ªæ
        2.å¦‚æœå·²æ»¡ï¼šæ£€æŸ¥æ˜¯å¦è¢«å¼±æ”¯é…
        """
        # ç­–ç•¥1ï¼šä¿ç•™å¤šæ ·æ€§
        if len(existing_labels) < self.max_labels_per_node:
            # åªæœ‰è¢«å¤šä¸ªæ ‡ç­¾æ˜ç¡®æ”¯é…æ—¶æ‰å‰ªæ
            domination_count = 0
            for existing in existing_labels:
                if existing.dominates_weak(label, alpha):
                    domination_count += 1
            
            # è¢«2ä¸ªä»¥ä¸Šæ ‡ç­¾æ”¯é…æ‰å‰ªæ
            return domination_count >= 2
        
        # ç­–ç•¥2ï¼šæ ‡ç­¾æ•°å·²æ»¡ï¼Œä½¿ç”¨å¼±æ”¯é…
        for existing in existing_labels:
            if existing.dominates_weak(label, alpha):
                return True
        
        return False
    
    def _prune_labels(self, labels: List[ReverseLabel], alpha: float) -> List[ReverseLabel]:
        """
        å½“æ ‡ç­¾æ•°è¶…é™æ—¶ï¼Œç§»é™¤æœ€å·®çš„æ ‡ç­¾
        
        æ’åºæ ‡å‡†ï¼š
        1.Q_{1-Î±}ï¼ˆä¸»è¦ï¼‰
        2.Meanï¼ˆæ¬¡è¦ï¼‰
        3.-Varï¼ˆå†æ¬¡è¦ï¼‰
        """
        if len(labels) <= self.max_labels_per_node:
            return labels
        
        # å¤šç›®æ ‡æ’åº
        def label_score(label):
            q = label.distribution.get_quantile(1 - alpha)
            return (q, label.mean_cache, -label.variance_cache)
        
        # ä¿ç•™æœ€ä¼˜çš„max_labels_per_nodeä¸ª
        sorted_labels = sorted(labels, key=label_score, reverse=True)
        return sorted_labels[:self.max_labels_per_node]

    def _format_path(self, path: List[int]) -> str:
        if len(path) <= 10:
            return ' â†’ '.join(map(str, path))
        return f"{' â†’ '.join(map(str, path[:5]))} â†’ ...â†’ {' â†’ '.join(map(str, path[-3:]))}"